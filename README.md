# sentencepiece_example
Examples of using sentencepiece for specific purpose

* Example 1 - disable `spm_train` tokenizes a predefined set of words into subwords

For a set of predefined set of words generated by the following code, saved as file `prefined_tokens.txt`:
```python
vocab_size=100
predefined_tokens = ['<image>', '</image>']
visual_tokens = [f"<|vision{i}|>" for i in range(vocab_size)]
predefined_tokens.extend(visual_tokens)
with open("prefined_tokens.txt", "w") as f:
  for token in predefined_tokens:
    print(token, file=f)
```

Additional NLP text file to be tokenized by `spm_train`, saved as file `text.txt`:
```bash
spm_train -h > text.txt 2>&1 
```

Tokenizing
```bash
spm_train \
--input=text.txt \
--model_prefix=tokenizer \
--vocab_size=130 \
--character_coverage=1.0 \
--split_by_whitespace=true \
--user_defined_symbols=$(cat prefined_tokens.txt | tr '\n' ',')
```

The above code will generate the following code:
```bash
spm_train_main.cc(283) [_status.ok()] Internal: /opt/github/sentencepiece/src/trainer_interface.cc(584)
[(static_cast<int>(required_chars_.size() + meta_pieces_.size())) <= (trainer_spec_.vocab_size())]
Vocabulary size is smaller than required_chars. 130 vs 176.
Increase vocab_size or decrease character_coverage with --character_coverage option.
Program terminated with an unrecoverable error.
```

Based on the above error, set the vocab_size >= 176 is okay.
```bash
spm_train \
--input=text.txt \
--model_prefix=tokenizer \
--vocab_size=180 \ # increase vocab_size for more effective encoding of input text
--character_coverage=1.0 \
--split_by_whitespace=true \
--user_defined_symbols=$(cat prefined_tokens.txt | tr '\n' ',')
```

Verifying the tokenizer on `text_encode.txt`.
```bash
cat text_encode.txt

How are you?
<image><|vision0|><|vision1|><|vision2|></image>.\nDescribe it briefly.
```

```bash
spm_encode --model tokenizer.model text_encode.txt 

▁ H o w ▁ a r e ▁ y o u ?
▁ <image> <|vision0|> <|vision1|> <|vision2|> </image> . \ n D e s c r i b e ▁ i t ▁ b r i e f l y .
```

```python
import sentencepiece as spm

# Load the trained model
sp = spm.SentencePieceProcessor(model_file='tokenizer.model')

# Test tokenization
sentence = "<image><|vision0|><|vision1|><|vision2|></image>.\nDescribe it briefly."
tokens = sp.encode(sentence, out_type=str)
print(tokens)
# Expected Output:
# ['▁', '<image>', '<|vision0|>', '<|vision1|>', '<|vision2|>', '</image>', '.', '▁', 'D', 'e', 's', 'c', 'r', 'i', 'b', 'e', '▁', 'i', 't', '▁', 'b', 'r', 'i', 'e', 'f', 'l', 'y', '.']
```
